{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ['XLA_PYTHON_MEM_FRACTION'] = '0.7'\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import numpy as np\n",
    "import optax\n",
    "from flax.linen.initializers import constant, orthogonal\n",
    "import functools\n",
    "from typing import Sequence, NamedTuple, Any, Dict\n",
    "from flax.training.train_state import TrainState\n",
    "import distrax\n",
    "import gymnax\n",
    "from gymnax.wrappers.purerl import FlattenObservationWrapper, LogWrapper\n",
    "from envs.aeroplanax import AeroPlanax\n",
    "\n",
    "\n",
    "class ScannedRNN(nn.Module):\n",
    "    @functools.partial(\n",
    "        nn.scan,\n",
    "        variable_broadcast=\"params\",\n",
    "        in_axes=0,\n",
    "        out_axes=0,\n",
    "        split_rngs={\"params\": False},\n",
    "    )\n",
    "    @nn.compact\n",
    "    def __call__(self, carry, x):\n",
    "        \"\"\"Applies the module.\"\"\"\n",
    "        rnn_state = carry\n",
    "        ins, resets = x\n",
    "        rnn_state = jnp.where(\n",
    "            resets[:, np.newaxis],\n",
    "            self.initialize_carry(*rnn_state.shape),\n",
    "            rnn_state,\n",
    "        )\n",
    "        new_rnn_state, y = nn.GRUCell(features=ins.shape[1])(rnn_state, ins)\n",
    "        return new_rnn_state, y\n",
    "\n",
    "    @staticmethod\n",
    "    def initialize_carry(batch_size, hidden_size):\n",
    "        # Use a dummy key since the default state init fn is just zeros.\n",
    "        cell = nn.GRUCell(features=hidden_size)\n",
    "        return cell.initialize_carry(jax.random.PRNGKey(0), (batch_size, hidden_size))\n",
    "\n",
    "\n",
    "class ActorCriticRNN(nn.Module):\n",
    "    action_dim: Sequence[int]\n",
    "    config: Dict\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, hidden, x):\n",
    "        if self.config[\"ACTIVATION\"] == \"relu\":\n",
    "            activation = nn.relu\n",
    "        else:\n",
    "            activation = nn.tanh\n",
    "        obs, dones = x\n",
    "        embedding = nn.Dense(\n",
    "            self.config[\"FC_DIM_SIZE\"], kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(obs)\n",
    "        embedding = activation(embedding)\n",
    "\n",
    "        rnn_in = (embedding, dones)\n",
    "        hidden, embedding = ScannedRNN()(hidden, rnn_in)\n",
    "\n",
    "        actor_mean = nn.Dense(\n",
    "            self.config[\"GRU_HIDDEN_DIM\"], kernel_init=orthogonal(2), bias_init=constant(0.0)\n",
    "        )(embedding)\n",
    "        actor_mean = activation(actor_mean)\n",
    "        actor_mean = nn.Dense(\n",
    "            self.action_dim, kernel_init=orthogonal(0.01), bias_init=constant(0.0)\n",
    "        )(actor_mean)\n",
    "        actor_logtstd = self.param(\"log_std\", nn.initializers.zeros, (self.action_dim,))\n",
    "        pi = distrax.MultivariateNormalDiag(actor_mean, jnp.exp(actor_logtstd))\n",
    "\n",
    "        critic = nn.Dense(\n",
    "            self.config[\"FC_DIM_SIZE\"], kernel_init=orthogonal(2), bias_init=constant(0.0)\n",
    "        )(embedding)\n",
    "        critic = activation(critic)\n",
    "        critic = nn.Dense(1, kernel_init=orthogonal(1.0), bias_init=constant(0.0))(\n",
    "            critic\n",
    "        )\n",
    "\n",
    "        return hidden, pi, jnp.squeeze(critic, axis=-1)\n",
    "\n",
    "\n",
    "class Transition(NamedTuple):\n",
    "    done: jnp.ndarray\n",
    "    action: jnp.ndarray\n",
    "    value: jnp.ndarray\n",
    "    reward: jnp.ndarray\n",
    "    log_prob: jnp.ndarray\n",
    "    obs: jnp.ndarray\n",
    "    info: jnp.ndarray\n",
    "\n",
    "def make_train(config):\n",
    "    config[\"NUM_UPDATES\"] = (\n",
    "        config[\"TOTAL_TIMESTEPS\"] // config[\"NUM_STEPS\"] // config[\"NUM_ENVS\"]\n",
    "    )\n",
    "    config[\"MINIBATCH_SIZE\"] = (\n",
    "        config[\"NUM_ENVS\"] * config[\"NUM_STEPS\"] // config[\"NUM_MINIBATCHES\"]\n",
    "    )\n",
    "    env = AeroPlanax()\n",
    "    env_params = env.default_params()\n",
    "    env = FlattenObservationWrapper(env)\n",
    "    env = LogWrapper(env)\n",
    "\n",
    "    def linear_schedule(count):\n",
    "        frac = (\n",
    "            1.0\n",
    "            - (count // (config[\"NUM_MINIBATCHES\"] * config[\"UPDATE_EPOCHS\"]))\n",
    "            / config[\"NUM_UPDATES\"]\n",
    "        )\n",
    "        return config[\"LR\"] * frac\n",
    "\n",
    "    def train(rng):\n",
    "        # INIT NETWORK\n",
    "        network = ActorCriticRNN(env.action_space(env_params).shape[0], config=config)\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        init_x = (\n",
    "            jnp.zeros(\n",
    "                (1, config[\"NUM_ENVS\"], *env.observation_space(env_params).shape)\n",
    "            ),\n",
    "            jnp.zeros((1, config[\"NUM_ENVS\"])),\n",
    "        )\n",
    "        init_hstate = ScannedRNN.initialize_carry(config[\"NUM_ENVS\"], 128)\n",
    "        network_params = network.init(_rng, init_hstate, init_x)\n",
    "        if config[\"ANNEAL_LR\"]:\n",
    "            tx = optax.chain(\n",
    "                optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]),\n",
    "                optax.adam(learning_rate=linear_schedule, eps=1e-5),\n",
    "            )\n",
    "        else:\n",
    "            tx = optax.chain(\n",
    "                optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]),\n",
    "                optax.adam(config[\"LR\"], eps=1e-5),\n",
    "            )\n",
    "        train_state = TrainState.create(\n",
    "            apply_fn=network.apply,\n",
    "            params=network_params,\n",
    "            tx=tx,\n",
    "        )\n",
    "\n",
    "        # INIT ENV\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        reset_rng = jax.random.split(_rng, config[\"NUM_ENVS\"])\n",
    "        obsv, env_state = jax.vmap(env.reset, in_axes=(0, None))(reset_rng, env_params)\n",
    "        init_hstate = ScannedRNN.initialize_carry(config[\"NUM_ENVS\"], 128)\n",
    "\n",
    "        # TRAIN LOOP\n",
    "        def _update_step(runner_state, unused):\n",
    "            # COLLECT TRAJECTORIES\n",
    "            def _env_step(runner_state, unused):\n",
    "                train_state, env_state, last_obs, last_done, hstate, rng = runner_state\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "\n",
    "                # SELECT ACTION\n",
    "                ac_in = (last_obs[np.newaxis, :], last_done[np.newaxis, :])\n",
    "                hstate, pi, value = network.apply(train_state.params, hstate, ac_in)\n",
    "                action = pi.sample(seed=_rng)\n",
    "                log_prob = pi.log_prob(action)\n",
    "                value, action, log_prob = (\n",
    "                    value.squeeze(0),\n",
    "                    action.squeeze(0),\n",
    "                    log_prob.squeeze(0),\n",
    "                )\n",
    "\n",
    "                # STEP ENV\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "                rng_step = jax.random.split(_rng, config[\"NUM_ENVS\"])\n",
    "                obsv, env_state, reward, done, info = jax.vmap(\n",
    "                    env.step, in_axes=(0, 0, 0, None)\n",
    "                )(rng_step, env_state, action, env_params)\n",
    "                transition = Transition(\n",
    "                    last_done, action, value, reward, log_prob, last_obs, info\n",
    "                )\n",
    "                runner_state = (train_state, env_state, obsv, done, hstate, rng)\n",
    "                return runner_state, transition\n",
    "\n",
    "            initial_hstate = runner_state[-2]\n",
    "            runner_state, traj_batch = jax.lax.scan(\n",
    "                _env_step, runner_state, None, config[\"NUM_STEPS\"]\n",
    "            )\n",
    "\n",
    "            # CALCULATE ADVANTAGE\n",
    "            train_state, env_state, last_obs, last_done, hstate, rng = runner_state\n",
    "            ac_in = (last_obs[np.newaxis, :], last_done[np.newaxis, :])\n",
    "            _, _, last_val = network.apply(train_state.params, hstate, ac_in)\n",
    "            last_val = last_val.squeeze(0)\n",
    "            def _calculate_gae(traj_batch, last_val, last_done):\n",
    "                def _get_advantages(carry, transition):\n",
    "                    gae, next_value, next_done = carry\n",
    "                    done, value, reward = transition.done, transition.value, transition.reward \n",
    "                    delta = reward + config[\"GAMMA\"] * next_value * (1 - next_done) - value\n",
    "                    gae = delta + config[\"GAMMA\"] * config[\"GAE_LAMBDA\"] * (1 - next_done) * gae\n",
    "                    return (gae, value, done), gae\n",
    "                _, advantages = jax.lax.scan(_get_advantages, (jnp.zeros_like(last_val), last_val, last_done), traj_batch, reverse=True, unroll=16)\n",
    "                return advantages, advantages + traj_batch.value\n",
    "            advantages, targets = _calculate_gae(traj_batch, last_val, last_done)\n",
    "\n",
    "            # UPDATE NETWORK\n",
    "            def _update_epoch(update_state, unused):\n",
    "                def _update_minbatch(train_state, batch_info):\n",
    "                    init_hstate, traj_batch, advantages, targets = batch_info\n",
    "\n",
    "                    def _loss_fn(params, init_hstate, traj_batch, gae, targets):\n",
    "                        # RERUN NETWORK\n",
    "                        _, pi, value = network.apply(\n",
    "                            params, init_hstate[0], (traj_batch.obs, traj_batch.done)\n",
    "                        )\n",
    "                        log_prob = pi.log_prob(traj_batch.action)\n",
    "\n",
    "                        # CALCULATE VALUE LOSS\n",
    "                        value_pred_clipped = traj_batch.value + (\n",
    "                            value - traj_batch.value\n",
    "                        ).clip(-config[\"CLIP_EPS\"], config[\"CLIP_EPS\"])\n",
    "                        value_losses = jnp.square(value - targets)\n",
    "                        value_losses_clipped = jnp.square(value_pred_clipped - targets)\n",
    "                        value_loss = (\n",
    "                            0.5 * jnp.maximum(value_losses, value_losses_clipped).mean()\n",
    "                        )\n",
    "\n",
    "                        # CALCULATE ACTOR LOSS\n",
    "                        ratio = jnp.exp(log_prob - traj_batch.log_prob)\n",
    "                        gae = (gae - gae.mean()) / (gae.std() + 1e-8)\n",
    "                        loss_actor1 = ratio * gae\n",
    "                        loss_actor2 = (\n",
    "                            jnp.clip(\n",
    "                                ratio,\n",
    "                                1.0 - config[\"CLIP_EPS\"],\n",
    "                                1.0 + config[\"CLIP_EPS\"],\n",
    "                            )\n",
    "                            * gae\n",
    "                        )\n",
    "                        loss_actor = -jnp.minimum(loss_actor1, loss_actor2)\n",
    "                        loss_actor = loss_actor.mean()\n",
    "                        entropy = pi.entropy().mean()\n",
    "\n",
    "                        total_loss = (\n",
    "                            loss_actor\n",
    "                            + config[\"VF_COEF\"] * value_loss\n",
    "                            - config[\"ENT_COEF\"] * entropy\n",
    "                        )\n",
    "                        return total_loss, (value_loss, loss_actor, entropy)\n",
    "\n",
    "                    grad_fn = jax.value_and_grad(_loss_fn, has_aux=True)\n",
    "                    total_loss, grads = grad_fn(\n",
    "                        train_state.params, init_hstate, traj_batch, advantages, targets\n",
    "                    )\n",
    "                    train_state = train_state.apply_gradients(grads=grads)\n",
    "                    return train_state, total_loss\n",
    "\n",
    "                (\n",
    "                    train_state,\n",
    "                    init_hstate,\n",
    "                    traj_batch,\n",
    "                    advantages,\n",
    "                    targets,\n",
    "                    rng,\n",
    "                ) = update_state\n",
    "\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "                permutation = jax.random.permutation(_rng, config[\"NUM_ENVS\"])\n",
    "                batch = (init_hstate, traj_batch, advantages, targets)\n",
    "\n",
    "                shuffled_batch = jax.tree_util.tree_map(\n",
    "                    lambda x: jnp.take(x, permutation, axis=1), batch\n",
    "                )\n",
    "\n",
    "                minibatches = jax.tree_util.tree_map(\n",
    "                    lambda x: jnp.swapaxes(\n",
    "                        jnp.reshape(\n",
    "                            x,\n",
    "                            [x.shape[0], config[\"NUM_MINIBATCHES\"], -1]\n",
    "                            + list(x.shape[2:]),\n",
    "                        ),\n",
    "                        1,\n",
    "                        0,\n",
    "                    ),\n",
    "                    shuffled_batch,\n",
    "                )\n",
    "\n",
    "                train_state, total_loss = jax.lax.scan(\n",
    "                    _update_minbatch, train_state, minibatches\n",
    "                )\n",
    "                update_state = (\n",
    "                    train_state,\n",
    "                    init_hstate,\n",
    "                    traj_batch,\n",
    "                    advantages,\n",
    "                    targets,\n",
    "                    rng,\n",
    "                )\n",
    "                return update_state, total_loss\n",
    "\n",
    "            init_hstate = initial_hstate[None, :]  # TBH\n",
    "            update_state = (\n",
    "                train_state,\n",
    "                init_hstate,\n",
    "                traj_batch,\n",
    "                advantages,\n",
    "                targets,\n",
    "                rng,\n",
    "            )\n",
    "            update_state, loss_info = jax.lax.scan(\n",
    "                _update_epoch, update_state, None, config[\"UPDATE_EPOCHS\"]\n",
    "            )\n",
    "            train_state = update_state[0]\n",
    "            metric = traj_batch.info\n",
    "            rng = update_state[-1]\n",
    "            if config.get(\"DEBUG\"):\n",
    "\n",
    "                def callback(info):\n",
    "                    return_values = info[\"returned_episode_returns\"][\n",
    "                        info[\"returned_episode\"]\n",
    "                    ]\n",
    "                    timesteps = (\n",
    "                        info[\"timestep\"][info[\"returned_episode\"]] * config[\"NUM_ENVS\"]\n",
    "                    )\n",
    "                    for t in range(len(timesteps)):\n",
    "                        print(\n",
    "                            f\"global step={timesteps[t]}, episodic return={return_values[t]}\"\n",
    "                        )\n",
    "\n",
    "                jax.debug.callback(callback, metric)\n",
    "\n",
    "            runner_state = (train_state, env_state, last_obs, last_done, hstate, rng)\n",
    "            return runner_state, metric\n",
    "\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        runner_state = (\n",
    "            train_state,\n",
    "            env_state,\n",
    "            obsv,\n",
    "            jnp.zeros((config[\"NUM_ENVS\"]), dtype=bool),\n",
    "            init_hstate,\n",
    "            _rng,\n",
    "        )\n",
    "        runner_state, metric = jax.lax.scan(\n",
    "            _update_step, runner_state, None, config[\"NUM_UPDATES\"]\n",
    "        )\n",
    "        return {\"runner_state\": runner_state, \"metric\": metric}\n",
    "\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"LR\": 2.5e-4,\n",
    "    \"NUM_ENVS\": 3000,\n",
    "    \"NUM_STEPS\": 3000,\n",
    "    \"TOTAL_TIMESTEPS\": 2e8,\n",
    "    \"FC_DIM_SIZE\": 128,\n",
    "    \"GRU_HIDDEN_DIM\": 128,\n",
    "    \"UPDATE_EPOCHS\": 16,\n",
    "    \"NUM_MINIBATCHES\": 5,\n",
    "    \"GAMMA\": 0.99,\n",
    "    \"GAE_LAMBDA\": 0.95,\n",
    "    \"CLIP_EPS\": 0.2,\n",
    "    \"ENT_COEF\": 1e-3,\n",
    "    \"VF_COEF\": 1,\n",
    "    \"MAX_GRAD_NORM\": 2,\n",
    "    \"ACTIVATION\": \"tanh\",\n",
    "    \"ANNEAL_LR\": True,\n",
    "}\n",
    "\n",
    "rng = jax.random.PRNGKey(42)\n",
    "train_jit = jax.jit(make_train(config))\n",
    "out = train_jit(rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "import matplotlib.pyplot as plt\n",
    "# rng = jax.random.PRNGKey(42)\n",
    "# t0 = time.time()\n",
    "# out = jax.block_until_ready(train_jit(rng))\n",
    "# print(f\"time: {time.time() - t0:.2f} s\")\n",
    "plt.plot(out[\"metric\"][\"returned_episode_returns\"].mean(-1).reshape(-1))\n",
    "plt.xlabel(\"Update Step\")\n",
    "plt.ylabel(\"Return\")\n",
    "plt.show()\n",
    "plt.cla()\n",
    "plt.plot(out[\"metric\"][\"returned_episode_lengths\"].mean(-1).reshape(-1))\n",
    "plt.xlabel(\"Update Step\")\n",
    "plt.ylabel(\"Return\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aeroplanax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
